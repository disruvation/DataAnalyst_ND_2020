{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Report on Wrangling and Analyzing WeRateDogs Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "This project is about wrangling and analyzing the WeRateDogs Twitter data. In this report I will briefly describe my wrangling efforts. Therefore I will describe how I gathered the data from different sources, assessed the data and at the end cleaned the data that they can be used for analysis. The acutal work which is described here in words can be found in the document: wrangle_act.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gathering'></a>\n",
    "## Gathering Data\n",
    "\n",
    "Within this project three different kinds of data need to be gathered. For each kind of the data a different method will be used. \n",
    "\n",
    "### Gathering Data WeRateDogs twitter archive by reading a csv file\n",
    "\n",
    "The first data file (twitter_archive_enhanced.csv) was manually downloaded and saved in the same folder as the Jupyter notebook.\n",
    "\n",
    "### Gathering image data by downloading a file using the Request library\n",
    "\n",
    "Afterward I downloaded the corresponding image data from an Udacity server. \n",
    "The Request library was used to download the file programmatically from the following URL: <br>\n",
    "https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\n",
    "\n",
    "### Gathering JSON data using a twitter API called Tweepy library\n",
    "\n",
    "The last part of these section deals with API handling and I needed to download the JSON data of the tweeds within the WeRateDogs archive. PythonÂ´s Tweepy library was used to download and store the the entire set of JSON data in a file named: tweet_json.txt ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assessing'></a>\n",
    "## Assessing Data\n",
    "After gathering the data, I assessed it to identify quality and tidiness issues.\n",
    "The following tables summarizes the found issues: <br>\n",
    "\n",
    "### Summarizing quality issues\n",
    "\n",
    "| Issue Number | table name | Issues decription | Comment |\n",
    "|----|----|----|----|\n",
    "| Q1 | df_twitter_archive | 181 retwitted data should be deleted, because they are not relevant for original tweets |  |\n",
    "| Q2 | df_twitter_archive | 78 in_reply data should be deleted, because they are not relevant for original tweets |  |\n",
    "| Q3 | df_twitter_archive | tweet_id should be a string, not an int |  |\n",
    "| Q4 | df_twitter_archive | timestamp should be datetime, not a string |  |\n",
    "| Q5 | df_twitter_archive | missing or incorrect dog name -> 'None', 'a', 'an, 'the', 'my' |  |\n",
    "| Q6 | df_twitter_archive | Nearly 84% of the dogs have no stage (all 4 stages are \"None\") | Will not be cleaned, because it would remove ~ 84% of the rows |\n",
    "| Q7 | df_image_pred | tweet_id should be a string, not an int |  |\n",
    "| Q8 | df_image_pred | columns should be renamed for an easier understanding of the content |  |\n",
    "| Q9 | df_image_pred | 324 entries predict no dog for all three predictions. |  |\n",
    "| Q10 | df_twitter_add_data | retweet_count should be an int, not a string |  |\n",
    "| Q11 | df_twitter_add_data | favorite_count should be an int, not a string |  |\n",
    "\n",
    "### Summarizing tidiness issues\n",
    "\n",
    "| Issue Number | table name | Issues decription | Comment |\n",
    "|----|----|----|----|\n",
    "| T1 | df_twitter_archive | dog stages should be merged to one column | - |\n",
    "| T2 | df_twitter_archive | retweeted columns (retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp) can be deleted, because they are not relevant for original tweets | - |\n",
    "| T3 | df_twitter_archive | reply columns (in_reply_to_status_id, in_reply_to_user_id) can be deleted, because they are not relevant for original tweets | - |\n",
    "| T4 | general | all 3 data frames should be merged into one | - |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cleaning'></a>\n",
    "## Cleaning Data\n",
    "After identifying the quality and tidiness issues, I started to clean the data and resolved issue by issue. Therefore I had to use my Python knowledge and what I learned so far within this Nanodegree course. In addition there were some tricky parts for which I needed some help from stackoverflow or the pandas documentation. All the used references are linked in the corresponding reference section in the working document wrangle_act.ipynb. <br>\n",
    "\n",
    "\n",
    "After all these steps, the data were ready for an analysis. The results of the analysis using visualizations is shown in the report: act_report.html ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "## References\n",
    "Markdown Tables: https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#tables <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
